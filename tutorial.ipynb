{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¤’ Epidemic mitigation project - environment tutorial\n",
    "\n",
    "This notebook is a tutorial on how to interact with the *epidemic simulation environment* provided for the miniproject.\n",
    "\n",
    "## Installing the environment\n",
    "\n",
    "1. Install conda if you don't have it (see [the conda documentation here](https://conda.io/projects/conda/en/latest/user-guide/install/index.html) to learn how to do so)\n",
    "2. Create a dedicated environment and install the packages in the environment by running the following commands:\n",
    "```\n",
    "conda create -n epi\n",
    "conda activate epi\n",
    "conda install pytorch torchvision torchaudio -c pytorch\n",
    "conda install numpy networkx matplotlib pyyaml jupyter tqdm pandas\n",
    "pip install gym\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the relevant packages\n",
    "If the environment is correctly setup you should be able to sucessfully import the following packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from gym import spaces\n",
    "\n",
    "\"\"\"Environment imports\"\"\"\n",
    "from epidemic_env.env       import Env, Log\n",
    "from epidemic_env.dynamics  import ModelDynamics, Observation\n",
    "from epidemic_env.visualize import Visualize\n",
    "from epidemic_env.agent     import Agent\n",
    "\n",
    "\"\"\"Pytorch and numpy imports\"\"\"\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the environment object and running it\n",
    "In the following cell we instanciate a dynamical model (of the `ModelDynamics` class) that implements the epidemic simulation. The simulation parameters are set by the config file that we load (here `switzerland.yaml`). We then demonstrate how one can query the list of cities from the dynamical model and how one can plot the map on which the simulation will happen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyn = ModelDynamics('config/switzerland.yaml')   # load the switzerland map\n",
    "print(dyn.cities)\n",
    "dyn.draw_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the dots represents the population in each city, the width of the connecting edges represents how fast contamination can propagate to one city to the next. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instanciating the environment object\n",
    "The environment provides a wrapper for an agent to interact with. It implements a step method that allows an agent to make observations and pick actions that will impact the environment. We show how to reset the environment (which is also the procedure used for initialization and for *seeding* the random number generators)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Loading the environment\"\"\"\n",
    "env = Env(  dyn, # We pass the dynamical model to the environment \n",
    "            action_space=None, # Here one could pass an openai gym action space that can then be sampled\n",
    "            observation_space=None, # Here one could pass an openai gym obs space that can then be sampled\n",
    "            )\n",
    "\n",
    "\"\"\" Resetting the environment \"\"\"\n",
    "obs, info = env.reset(seed=0) # We pass a seed to the env to ensure reproductibility \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stepping throught the environment\n",
    "Actions are passed to the environment as a dictionary of booleans. In the following cell we create an example `null` action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = { # DO NOTHING\n",
    "        'confinement': False, \n",
    "        'isolation': False, \n",
    "        'hospital': False, \n",
    "        'vaccinate': False,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an action dictionary, we can pass it to the environment and perform an environment *step*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"One step amounts to one week in the simulation environment\"\"\"\n",
    "obs, reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each step the environment returns four variables :\n",
    "1. An observation `obs` (to be used by the agent for decision making)\n",
    "2. A reward `rew` (to be used for training)\n",
    "3. A boolean variable `done` (indicates when an episode is finished)\n",
    "4. An information object `info` (to be used for policy interpretation and debugging)\n",
    "\n",
    "we will now show in details what each of those variables contain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the observations object\n",
    "The observation object is a dataclass containing all the information that the agent is allowed to access to make its decisions. In the following cells we show how to access the data in the observation object. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pop` attribute of the observation gives the full population of each city (before the epidemic started). This is meant to be used to scale the observations when feeding them to a neural network. The `pop` attribute is a dictionary with string keys and int values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Full population\")\n",
    "print(f\"     {obs.pop}\")\n",
    "obs.pop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When iterating through cities, it is useful to have a full list of the cities. Such a list can be accessed through the dynamical model object `dyn.cities` attribute. Below we print the population of each city using that method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Initial population in each city:')\n",
    "for c in dyn.cities:\n",
    "    print(f\"     {c} : {obs.pop[c]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `total` attribute gives the number of infected and dead people in the entire country. This is measured for each day of the week SINCE THE BEGINNING, OR JUST THIS WWEK? DEATHS = ACCUMULATED OVER THE WEEK?   # TODO. Below we get the total observation and then use it to plot the infected and death counts evolution for the week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Full country observation\")\n",
    "print(f\"     {obs.total}\")\n",
    "fig, ax = plt.subplots(2,figsize=(10,5))\n",
    "ax[0].plot(obs.total.dead)\n",
    "ax[0].set_title('deaths')\n",
    "ax[1].plot(obs.total.infected)\n",
    "ax[1].set_title('infected')\n",
    "ax[1].set_xlabel('time in days')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `city` attribute contains a dictionnary that allows acessing each city's number of infected and dead people. Each element of that dict is structured exactly as the `total` attribute is structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lausanne's observation\")\n",
    "print(f\"     {obs.city['Lausanne']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the informations object\n",
    "The information object is a dataclass object that contains the most important parameters of the dynamical system as it unfolds. In the following cells we show how to read its content for plotting, analysing and debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the observation dictionary, the information dictionnary contains the actions that were taken during the past week. They can be recovered as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info.action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the information object contains `info.total` and `info.city` attributes (which as structured in a similar way to how the `obs.total` and `obs.city` (the city attribute is a dict, the total attribute contains the value for the full country). \n",
    "\n",
    "Unlike the observation attribute the information contains *all simulation variables* but *only on the last day of the week*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info.total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarily, one can access the attributes for one specific city (here the number of recovered people in Bern) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info.city['Bern'].recovered "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing an example agent, going through an entire episode\n",
    "The following section shows how to implement a dummy agent. It introduces the action and observation preprocessors. We recommand that you subclass the example `Agent` class that is provided with the environment. Because the most efficient way to represent actions for a neural network is not the clearest way to represent actions for the environment, we recommand working with *action preprocessors* and *observation preprocessors*. The following cells detail how to implement them.\n",
    "\n",
    "![](figures/preprocessing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Properly defining the action and observation spaces will allow us to sample random actions, observations from them easily. This will be useful to automatically define the size of our neural networks, and to implement epsilon-greedy exploration schemes. In order to better understand the action and observation space, read the corresponding sections in the [OpenAI Gym Documentation](https://www.gymlibrary.dev/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space        =   spaces.Discrete(5)\n",
    "observation_space   =   spaces.Box( low=0,\n",
    "                                    high=1,\n",
    "                                    shape=(2, dyn.n_cities, dyn.env_step_length),\n",
    "                                    dtype=np.float16)\n",
    "print(f\"sampled action : {action_space.sample()}\")\n",
    "print(\"Sampled observation\")\n",
    "plt.matshow(observation_space.sample()[0,:,:])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell we implement preprocessing functions for actions and observations. The action preprocessing function takes an int and returns an action dictionary. The observation preprocessing function takes an observation object, and returns a torch tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALE = 100\n",
    "ACTION_NULL = 0\n",
    "ACTION_CONFINE = 1\n",
    "ACTION_ISOLATE = 2\n",
    "ACTION_HOSPITAL = 3\n",
    "ACTION_VACCINATE = 4\n",
    "\n",
    "\n",
    "def action_preprocessor(a:torch.Tensor, dyn:ModelDynamics):\n",
    "    action = { # DO NOTHING\n",
    "        'confinement': False, \n",
    "        'isolation': False, \n",
    "        'hospital': False, \n",
    "        'vaccinate': False,\n",
    "    }\n",
    "    \n",
    "    if a == ACTION_CONFINE:\n",
    "        action['confinement'] = True\n",
    "    elif a == ACTION_ISOLATE:\n",
    "        action['isolation'] = True\n",
    "    elif a == ACTION_VACCINATE:\n",
    "        action['vaccinate'] = True\n",
    "    elif a == ACTION_HOSPITAL:\n",
    "        action['hospital'] = True\n",
    "        \n",
    "    return action\n",
    "    \n",
    "def observation_preprocessor(obs: Observation, dyn:ModelDynamics):\n",
    "    infected = SCALE * np.array([np.array(obs.city[c].infected)/obs.pop[c] for c in dyn.cities])\n",
    "    dead = SCALE * np.array([np.array(obs.city[c].infected)/obs.pop[c] for c in dyn.cities])\n",
    "    confined = np.ones_like(dead)*int((dyn.get_action()['confinement']))\n",
    "    return torch.Tensor(np.stack((infected, dead, confined))).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell we **instanciate an environment with set action and observation spaces attributes**. This will allow for sampling the env for example actions and observations, which we use for random actions in the example dummy agent. The action preprocessor and the observation preprocessor that we defined below are also passed to the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Env(  dyn,\n",
    "            action_space=action_space,\n",
    "            observation_space=observation_space,\n",
    "            action_preprocessor=action_preprocessor,\n",
    "            observation_preprocessor=observation_preprocessor,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell we show how to subclass the agent class. (Here to create a fully random dummy agent with 5 actions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleAgent(Agent):\n",
    "    def __init__(self,  env:Env,\n",
    "                # Additionnal parameters to be added here\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Example agent implementation. Just picks a random action at each time step.\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        \n",
    "    def load_model(self, savepath):\n",
    "        # This is where one would define the routine for loading a pre-trained model\n",
    "        pass\n",
    "\n",
    "    def save_model(self, savepath):\n",
    "        # This is where one would define the routine for saving the weights for a trained model\n",
    "        pass\n",
    "\n",
    "    def optimize_model(self):\n",
    "        # This is where one would define the optimization step of an RL algorithm\n",
    "        return 0\n",
    "    \n",
    "    def reset(self,):\n",
    "        # This should be called when the environment is reset\n",
    "        pass\n",
    "    \n",
    "    def act(self, obs):\n",
    "        # this takes an observation and returns an action\n",
    "        # the action space can be directly sampled from the env\n",
    "        return self.env.action_space.sample() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running through an episode with the dummy agent\n",
    "The following cell provides an example of going through an entire episode while logging the information dictionary as it is produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ExampleAgent(env)\n",
    "\n",
    "\"\"\" Run the simulation \"\"\"\n",
    "log = []\n",
    "finished = False\n",
    "obs, info = env.reset(2)\n",
    "agent.reset()\n",
    "agent.epsilon = 0\n",
    "while not finished:\n",
    "    action = agent.act(obs)\n",
    "    obs, R, finished, info = env.step(action)\n",
    "    log.append(info) # save the information dict for logging\n",
    "    if finished:\n",
    "        break\n",
    "    \n",
    "\n",
    "\"\"\" Parse the logs \"\"\"\n",
    "total = {p:np.array([getattr(l.total,p) for l in log]) for p in dyn.parameters}\n",
    "cities = {c:{p:np.array([getattr(l.city[c],p) for l in log]) for p in dyn.parameters} for c in dyn.cities}\n",
    "actions = {a:np.array([l.action[a] for l in log]) for a in log[0].action.keys()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell we provide an example of plotting an episode, from the log dictionnaries defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "fig = plt.figure(figsize=(14,10))\n",
    "ax_leftstate = plt.subplot2grid(shape=(9, 2), loc=(0, 0), rowspan=4)\n",
    "ax_leftobs = plt.subplot2grid(shape=(9, 2), loc=(4, 0), rowspan=3)\n",
    "ax_leftactions = plt.subplot2grid(shape=(9, 2), loc=(7, 0), rowspan=2)\n",
    "ax_right = [plt.subplot2grid(shape=(9, 2), loc=(0, 1), colspan=1)]\n",
    "ax_right += [plt.subplot2grid(shape=(9, 2), loc=(i, 1), colspan=1) for i in range(1,9)]\n",
    "ax_right = {k:ax_right[_id] for _id,k in enumerate(cities.keys())}\n",
    "\n",
    "[ax_leftstate.plot(y) for y in total.values()]\n",
    "ax_leftstate.legend(total.keys())\n",
    "ax_leftstate.set_title('Full state')\n",
    "ax_leftstate.set_ylabel('number of people in each state')\n",
    "\n",
    "[ax_leftobs.plot(total[y]) for y in ['infected','dead']]\n",
    "ax_leftobs.legend(['infected','dead'])\n",
    "ax_leftobs.set_title('Observable state')\n",
    "ax_leftobs.set_ylabel('number of people in each state')\n",
    "\n",
    "ax_leftactions.imshow(np.array([v for v in actions.values()]).astype(np.uint8),aspect='auto')\n",
    "ax_leftactions.set_title('Actions')\n",
    "ax_leftactions.set_yticks([0,1,2,3])\n",
    "ax_leftactions.set_yticklabels(list(actions.keys()))\n",
    "ax_leftactions.set_xlabel('time (in weeks)')\n",
    "\n",
    "[ax.plot(cities[c]['infected']) for c, ax in ax_right.items()]\n",
    "[ax.plot(cities[c]['dead']) for c, ax in ax_right.items()]\n",
    "[ax.set_ylabel(c) for c, ax in ax_right.items()]\n",
    "[ax.xaxis.set_major_locator(plt.NullLocator()) for c, ax in ax_right.items()]\n",
    "ax_right['ZÃ¼rich'].set_xlabel('time (in weeks)')\n",
    "ax_right['ZÃ¼rich'].xaxis.set_major_locator(MultipleLocator(2.000))\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinism, pseudo-randomness and reproducibility\n",
    "We ask you to ensure that your results are reproducible. To make sure this is the case you need to learn how to seed the environment, as well as the pytorch random number generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making sure that we get the same initialization twice for the environment can be done by seeding the reset function (when resetting the environment). In the cell below we show how seeding the environment ensures episodes are reproductible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyn = ModelDynamics('config/switzerland.yaml') #load the switzerland map\n",
    "\n",
    "env = Env(  dyn, # We pass the dynamical model to the environment \n",
    "            action_space=None, # Here one could pass an openai gym action space that can then be sampled\n",
    "            observation_space=None, # Here one could pass an openai gym obs space that can then be sampled\n",
    "            )\n",
    "\n",
    "action = { # DO NOTHING\n",
    "        'confinement': False, \n",
    "        'isolation': False, \n",
    "        'hospital': False, \n",
    "        'vaccinate': False,\n",
    "    }\n",
    "\n",
    "fig, ax = plt.subplots(3,2,figsize=(10,10))\n",
    "\n",
    "\"\"\" Run the simulations, unseeded\"\"\"\n",
    "for trace in range(3):\n",
    "    log = []\n",
    "    finished = False\n",
    "    obs, info = env.reset()\n",
    "    for t in range(30):\n",
    "        obs, R, finished, info = env.step(action)\n",
    "        log.append(info) # save the information dict for logging\n",
    "        if finished:\n",
    "            break\n",
    "    \"\"\" Parse the logs \"\"\"\n",
    "    total = {p:np.array([getattr(l.total,p) for l in log]) for p in dyn.parameters}\n",
    "    cities = {c:{p:np.array([getattr(l.city[c],p) for l in log]) for p in dyn.parameters} for c in dyn.cities}\n",
    "    actions = {a:np.array([l.action[a] for l in log]) for a in log[0].action.keys()}\n",
    "\n",
    "    [ax[trace,0].plot(y) for y in total.values()]\n",
    "    ax[trace,0].legend(total.keys())\n",
    "    ax[trace,0].set_title(f'Unseeded example {trace}')\n",
    "    ax[trace,0].set_ylabel('number of people in each state')\n",
    "\n",
    "\n",
    "seed = 0\n",
    "\"\"\" Run the simulations, seeded\"\"\"\n",
    "for trace in range(3):\n",
    "    log = []\n",
    "    finished = False\n",
    "    obs, info = env.reset(seed)\n",
    "    for t in range(30):\n",
    "        obs, R, finished, info = env.step(action)\n",
    "        log.append(info) # save the information dict for logging\n",
    "        if finished:\n",
    "            break\n",
    "    \"\"\" Parse the logs \"\"\"\n",
    "    total = {p:np.array([getattr(l.total,p) for l in log]) for p in dyn.parameters}\n",
    "    cities = {c:{p:np.array([getattr(l.city[c],p) for l in log]) for p in dyn.parameters} for c in dyn.cities}\n",
    "    actions = {a:np.array([l.action[a] for l in log]) for a in log[0].action.keys()}\n",
    "\n",
    "    [ax[trace,1].plot(y) for y in total.values()]\n",
    "    ax[trace,1].legend(total.keys())\n",
    "    ax[trace,1].set_title(f'Seeded example {trace}')\n",
    "    ax[trace,1].set_ylabel('number of people in each state')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure reproducible training, we also must make sure that the parameters of our neural networks are initialized to the same values from one run to the next. To do so, we need to seed the pytorch random number generator. This is performed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unseeded random number generation, using non-determinstic algs\")\n",
    "for _ in range(3):\n",
    "    torch.use_deterministic_algorithms(False)\n",
    "    print(f\"    rnd : {torch.randint(0,10,(2,))}\")\n",
    "    \n",
    "print(\"Seeded random number generation, using only determinstic algs\")\n",
    "for _ in range(3):\n",
    "    torch.manual_seed(0)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    print(f\"    rnd : {torch.randint(0,10,(2,))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram functions\n",
    "In order to facilitate the plotting of the evaluation runs, we give you an example function with pre-set bins so your results are easily comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create mock data \"\"\"\n",
    "deaths = np.random.normal(7.5e4,1e4,size=(50,1))\n",
    "rewards = np.random.normal(2,20,size=(50,1))\n",
    "conf_days = np.random.normal(40,20,size=(50,1))\n",
    "isol_days = np.random.normal(22,30,size=(50,1))\n",
    "vaccination_days = np.random.normal(44,2,size=(50,1))\n",
    "hospital_days = np.random.normal(110,10,size=(50,1))\n",
    "\n",
    "\"\"\" Plot example \"\"\"\n",
    "fig, ax = plt.subplots(3,2,figsize=(18,8))\n",
    "def hist_avg(ax, data,title):\n",
    "    ymax = 50\n",
    "    if title == 'deaths':\n",
    "        x_range = (1000,200000)\n",
    "    elif title == 'cumulative rewards': \n",
    "        x_range = (-300,300)\n",
    "    elif 'days' in title:\n",
    "        x_range = (0,200)\n",
    "    else:\n",
    "        raise ValueError(f'{title} is not a valid title') \n",
    "    ax.set_title(title)\n",
    "    ax.set_ylim(0,ymax)\n",
    "    ax.vlines([np.mean(data)],0,ymax,color='red')\n",
    "    ax.hist(data,bins=60,range=x_range)\n",
    "hist_avg(ax[0,0], deaths,'deaths')\n",
    "hist_avg(ax[1,0], rewards,'cumulative rewards')\n",
    "hist_avg(ax[2,0], conf_days,'confined days')\n",
    "hist_avg(ax[0,1], isol_days,'isolation days')\n",
    "hist_avg(ax[1,1], vaccination_days,'vaccination days')\n",
    "hist_avg(ax[2,1], hospital_days,'hospital days')\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\"\"\" Print example \"\"\"\n",
    "print(f'Average death number: {np.mean(deaths)}')\n",
    "print(f'Average number of confined days: {np.mean(conf_days)}')\n",
    "print(f'Average number of isolation days: {np.mean(conf_days)}')\n",
    "print(f'Average number of additional hospital days: {np.mean(hospital_days)}')\n",
    "print(f'Average number of vaccination: {np.mean(vaccination_days)}')\n",
    "print(f'Average cumulative reward: {np.mean(rewards)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "5add61fee41a7b1e2f73956a71011f2780b5879f5bd7bc6276eaf5a5bcc93f57"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
